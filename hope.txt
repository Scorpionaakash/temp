#Design a simple linear neural network model.
#include<iostream.h>
#include<conio.h>
void main() 
{ 
clrscr(); 
float x,b,w,net, out;
cout<<"\nEnter the input X=";
cin>>x;
cout<<"\nEnter the bias b=";
cin>>b;
cout<<"\nEnter the weight W=";
cin>>w;
net=(w*x+b);
cout<<"\n****************OutPut********************\n";
cout<<"\nnet="<<net<<endl;
if(net<0)
{  out=0;}
else
if((net>=0)&&(net<=1))
{out=net;}
else
out=1;
cout<<"output="<<out<<endl; 
getch(); 
} 

#using both binary and bipolar sigmoidal function

import numpy
n=int (input("Enter Number of Elements :"))
print("Enter the inputs ")
inputs=[]
for i in range(0,n):
    ele=float(input())
    inputs.append(ele) 
#adding the element
print(input)
print("Enter the weights:")
weights=[]
#iterating till the range
for i in range(0,n):
    ele=float(input())
    weights.append(ele)#adding the elemnt
print(weights)

print("The net input can be calculated as Yin=x1w1+x2w2+x3w3")

Yin=[]
for i in range(0,n):
    Yin.append(inputs[i]*weights[i])
print(round(sum(Yin),3))

#AND / NOT function for MP Neuron 

import numpy
# enter the no of inputs
num_ip = int(input("Enter the number of inputs : "))

#Set the weights with value 1
w1 = 1
w2 = 1

print("For the ", num_ip , " inputs calculate the net input using yin = x1w1 + x2w2 ")

x1 = []
x2 = []
for j in range(0, num_ip):
    ele1 = int(input("x1 = "))
    ele2 = int(input("x2 = "))
    x1.append(ele1)
    x2.append(ele2)
print("x1 = ",x1)
print("x2 = ",x2)

n = x1 * w1
m = x2 * w2

Yin = []
for i in range(0, num_ip):
	Yin.append(n[i] + m[i])
print("Yin = ",Yin)

#Assume one weight as excitatory and the other as inhibitory, i.e.,

Yin = []
for i in range(0, num_ip):
	Yin.append(n[i] - m[i])
print("After assuming one weight as excitatory and the other as inhibitory Yin = ",Yin)

#From the calculated net inputs, now it is possible to fire the neuron for input (1, 0)
#only by fixing a threshold of 1, i.e., θ ≥ 1 for Y unit. 
#Thus, w1 = 1, w2 = -1; θ ≥ 1

Y=[]

for i in range(0, num_ip):
    if(Yin[i]>=1):
     ele=  1
     Y.append(ele)
    if(Yin[i]<1):
     ele=  0
     Y.append(ele)
print("Y = ",Y)

#XOR function using MP neural net.
print("XOR function using Mc-Culloch Pitts neuron")	
print()
print("Enter 4 binary inputs.");
x1inputs=[]	
x2inputs=[]
c=input("Press 1 to enter inputs or Enter to use default inputs.")
if(c=="1"):
    for i in range(0,4):
            x1=int(input("Enter x1 : "))	
            x1inputs.append(x1)       
            x2=int(input("Enter x2 : "))	
            x2inputs.append(x2)        
else:
    x1inputs=[1,1,0,0]
    x2inputs=[1,0,1,0]

print("Calculating z1 = x1 x2'")
print("Considering one weight as excitatory and other as inhibitory.");
w1 = [1,1,1,1]	
w2 = [-1,-1,-1,-1]	
z1=[]          
for i in range(0,4):    
    z1.append(x1inputs[i]*w1[i] + x2inputs[i]*w2[i])
print("x1 " , "x2 " , "z1")
for i in range(0,4):    
    print(x1inputs[i] ," ",  x2inputs[i]," " ,  z1[i])
print("Calculating z2 = x1' x2")
print("Considering one weight as excitatory and other as inhibitory.");
w1 = [-1,-1,-1,-1]	
w2 = [1,1,1,1]	
z2=[]          
for i in range(0,4):    
    z2.append(x1inputs[i]*w1[i] + x2inputs[i]*w2[i])
print("x1 " , "x2 " , "z2")
for i in range(0,4):    
    print(x1inputs[i] ," " ,  x2inputs[i] ," ", z2[i])
print("Applying Threshold=1 for z1 and z2")
for i in range(0,4):
    if(z1[i]>=1):        
        z1[i]=1
    else:        
        z1[i]=0
if(z2[i]>=1):        
    z2[i]=1
else:        
    z2[i]=0
print("z1 ","z2")
for i in range(0,4):    
    print(z1[i] ," ",  z2[i])
y = []	
v1=1	
v2=1
for i in range(0,4):
    y.append( z1[i]*v1 + z2[i]*v2 )
print("x1" , "x2" , " y")
for i in range(0,4):    
    print(x1inputs[i] ," " ,  x2inputs[i] ," ", y[i])

#hebb rule
#include<iostream.h>
#include<conio.h>
int main()
{
clrscr();
int x[4][3],w[3],y[4][1],i,j,p;
for(i=0;i<3;++i)
w[i]=0;
for(p=0;p<4;++p)
x[p][0]=1;
for(p=0;p<4;++p) {
cout<<"Enter the input Pattern number "<<p<<" : \n";
for(i=0;i<3;++i)
cin>>x[p][i];
cout<<"Enter the corresponding output \n";
for(j=0;j<1;++j)
cin>>y[p][j];
for(i=0;i<3;++i)
for(j=0;j<1;++j){
w[i]+=x[p][i]*y[p][j];
cout<<endl;

}
}
cout<<"The final weights = [ "<<w[0]<<" "<<w[1]<<" "<<w[2]<<" ]"<<endl;
getch();
return 0;
}

#Delta rule
##perceptron.py file
import numpy as np

class Perceptron(object):

    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):
        self.threshold = threshold
        self.learning_rate = learning_rate
        self.weights = np.zeros(no_of_inputs + 1)
        print("no_of_inputs",no_of_inputs)   
    def predict(self, inputs):
        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]
        if summation > 0:
          activation = 1
        else:
          activation = 0
      
        return activation

    def train(self, training_inputs, labels):
        
        for _ in range(self.threshold):
            for inputs, label in zip(training_inputs, labels):
                prediction = self.predict(inputs)
                self.weights[1:] += self.learning_rate * (label - prediction) * inputs
                self.weights[0] += self.learning_rate * (label - prediction)

## startDelta.py file
import numpy as np
from perceptron import Perceptron

training_inputs = []
training_inputs.append(np.array([1, 1]))
training_inputs.append(np.array([1, -1]))
training_inputs.append(np.array([-1, 1]))
training_inputs.append(np.array([-1, -1]))
print(training_inputs)
labels = np.array([-1, 1, -1, -1])
print(labels)
perceptron = Perceptron(2)
perceptron.train(training_inputs, labels)
print(perceptron.weights)
inputs = np.array([1, 1])
print(perceptron.predict(inputs)) 
#=> 1

inputs = np.array([-1, 1])
print(perceptron.predict(inputs))

##Back Propogation

from math import exp
from random import seed
from random import random

# Initialize a network
def initialize_network(n_inputs, n_hidden, n_outputs):
	network = list()
	hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]
	network.append(hidden_layer)
	output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]
	network.append(output_layer)
	return network

# Calculate neuron activation for an input
def activate(weights, inputs):
	activation = weights[-1]
	for i in range(len(weights)-1):
		activation += weights[i] * inputs[i]
	return activation

# Transfer neuron activation
def transfer(activation):
	return 1.0 / (1.0 + exp(-activation))

# Forward propagate input to a network output
def forward_propagate(network, row):
	inputs = row
	for layer in network:
		new_inputs = []
		for neuron in layer:
			activation = activate(neuron['weights'], inputs)
			neuron['output'] = transfer(activation)
			new_inputs.append(neuron['output'])
		inputs = new_inputs
	return inputs

# Calculate the derivative of an neuron output
def transfer_derivative(output):
	return output * (1.0 - output)

# Backpropagate error and store in neurons
def backward_propagate_error(network, expected):
	for i in reversed(range(len(network))):
		layer = network[i]
		errors = list()
		if i != len(network)-1:
			for j in range(len(layer)):
				error = 0.0
				for neuron in network[i + 1]:
					error += (neuron['weights'][j] * neuron['delta'])
				errors.append(error)
		else:
			for j in range(len(layer)):
				neuron = layer[j]
				errors.append(expected[j] - neuron['output'])
		for j in range(len(layer)):
			neuron = layer[j]
			neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])

# Update network weights with error
def update_weights(network, row, l_rate):
	for i in range(len(network)):
		inputs = row[:-1]
		if i != 0:
			inputs = [neuron['output'] for neuron in network[i - 1]]
		for neuron in network[i]:
			for j in range(len(inputs)):
				neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]
			neuron['weights'][-1] += l_rate * neuron['delta']

# Train a network for a fixed number of epochs
def train_network(network, train, l_rate, n_epoch, n_outputs):
	for epoch in range(n_epoch):
		sum_error = 0
		for row in train:
			outputs = forward_propagate(network, row)
			expected = [0 for i in range(n_outputs)]
			expected[row[-1]] = 1
			sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])
			backward_propagate_error(network, expected)
			update_weights(network, row, l_rate)
		print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))

# Test training backprop algorithm
seed(1)
dataset = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]
n_inputs = len(dataset[0]) - 1
n_outputs = len(set([row[-1] for row in dataset]))
network = initialize_network(n_inputs, 2, n_outputs)
train_network(network, dataset, 0.5, 20, n_outputs)
for layer in network:
	print(layer)

#Hopefield Network

##hopp.h

#include<math.h>
class neuron
{
protected:
     int activation;
     friend class network;
public:
     int weightv[4];
     neuron() {};
     neuron(int *j) ;
     int act(int, int*);
};

class network

{
public:
     neuron   nrn[4];
     int output[4];
     int threshld(int) ;
     void activation(int j[4]);
     network(int*,int*,int*,int*);
}; 

##HoppMain.cpp

//Single layer Hopfield Network with 4 neurons
#include "hopp.h"
#include<iostream.h>
#include<conio.h>
neuron::neuron(int *j)
{
int i;
for(i=0;i<4;i++)
     {
     weightv[i]= *(j+i);
     }
}

int neuron::act(int m, int *x)
{
int i;
int a=0;

for(i=0;i<m;i++)
     {
     a += x[i]*weightv[i];
     }
return a;
}

int network::threshld(int k)
{
if(k>=0)
     return (1);
else
     return (0);
}

network::network(int a[4],int b[4],int c[4],int d[4])
{
nrn[0] = neuron(a) ;
nrn[1] = neuron(b) ;
nrn[2] = neuron(c) ;
nrn[3] = neuron(d) ;
}

void network::activation(int *patrn)
{
int i,j;

for(i=0;i<4;i++)
     {
     for(j=0;j<4;j++)
	  {
	  cout<<"\n nrn["<<i<<"].weightv["<<j<<"] is "
	       <<nrn[i].weightv[j];
	  }
     nrn[i].activation = nrn[i].act(4,patrn);
     cout<<"\nactivation is "<<nrn[i].activation;
     output[i]=threshld(nrn[i].activation);
     cout<<"\noutput value is  "<<output[i]<<"\n";
     }
}

int main ()
{
clrscr();
int patrn1[]= {1,0,1,0},i;
int wt1[]= {0,-3,3,-3};
int wt2[]= {-3,0,-3,3};
int wt3[]= {3,-3,0,-3};
int wt4[]= {-3,3,-3,0};

cout<<"\nTHIS PROGRAM IS FOR A HOPFIELD NETWORK WITH A SINGLE LAYER OF";
cout<<"\n4 FULLY INTERCONNECTED NEURONS. THE NETWORK SHOULD RECALL THE";
cout<<"\nPATTERNS 1010 AND 0101 CORRECTLY.\n";

//create the network by calling its constructor.
// the constructor calls neuron constructor as many times as the number of
// neurons in the network.
network h1(wt1,wt2,wt3,wt4);

//present a pattern to the network and get the activations of the neurons
h1.activation(patrn1);

//check if the pattern given is correctly recalled and give message
for(i=0;i<4;i++)
     {
     if (h1.output[i] == patrn1[i])
	  cout<<"\n pattern= "<<patrn1[i]<<
	  "  output = "<<h1.output[i]<<"  component matches";
     else
	  cout<<"\n pattern= "<<patrn1[i]<<
	  "  output = "<<h1.output[i]<<
	  "  discrepancy occurred";
     }
cout<<"\n\n";
int patrn2[]= {0,1,0,1};
h1.activation(patrn2);
for(i=0;i<4;i++)
     {
     if (h1.output[i] == patrn2[i])
	  cout<<"\n pattern= "<<patrn2[i]<<
	  "  output = "<<h1.output[i]<<"  component matches";
     else
	  cout<<"\n pattern= "<<patrn2[i]<<
	  "  output = "<<h1.output[i]<<
	  "  discrepancy occurred";
       }
	 getch();
       return 0;
}


##KSOFM

from random import *
from math import *

class Node:

    def __init__(self, FV_size=10, PV_size=10, Y=0, X=0):
        self.FV_size=FV_size
        self.PV_size=PV_size
        self.FV = [0.0]*FV_size # Feature Vector
        self.PV = [0.0]*PV_size # Prediction Vector
        self.X=X # X location
        self.Y=Y # Y location
        
        for i in range(FV_size):
            self.FV[i]=random() # Assign a random number from 0 to 1
            
        for i in range(PV_size):
            self.PV[i]=random() # Assign a random number from 0 to 1


class SOM:

    #Let radius=False if you want to autocalculate the radis
    def __init__(self, height=10, width=10, FV_size=10, PV_size=10, radius=False, learning_rate=0.005):
        self.height=height
        self.width=width
        self.radius=radius if radius else (height+width)/2
        self.total=height*width
        self.learning_rate=learning_rate
        self.nodes=[0]*(self.total)
        self.FV_size=FV_size
        self.PV_size=PV_size
        for i in range(self.height):
            for j in range(self.width):
                self.nodes[(i)*(self.width)+j]=Node(FV_size, PV_size,i,j)

    # Train_vector format: [ [FV[0], PV[0]],
    #                        [FV[1], PV[1]], so on..
    
    def train(self, iterations=1000, train_vector=[[[0.0],[0.0]]]):
        time_constant=iterations/log(self.radius)
        radius_decaying=0.0
        learning_rate_decaying=0.0
        influence=0.0
        stack=[] #Stack for storing best matching unit's index and updated FV and PV
        temp_FV=[0.0]*self.FV_size
        temp_PV=[0.0]*self.PV_size
        for i in range(1,iterations+1):
            #print "Iteration number:",i
            radius_decaying=self.radius*exp(-1.0*i/time_constant)
            learning_rate_decaying=self.learning_rate*exp(-1.0*i/time_constant)
            print(i)
            
            for  j in range(len(train_vector)):
                input_FV=train_vector[j][0]
                input_PV=train_vector[j][1]
                best=self.best_match(input_FV)
                stack=[]
                for k in range(self.total):
                    dist=self.distance(self.nodes[best],self.nodes[k])
                    if dist < radius_decaying:
                        temp_FV=[0.0]*self.FV_size
                        temp_PV=[0.0]*self.PV_size
                        influence=exp((-1.0*(dist**2))/(2*radius_decaying*i))

                        for l in range(self.FV_size):
                            #Learning
                            temp_FV[l]=self.nodes[k].FV[l]+influence*learning_rate_decaying*(input_FV[l]-self.nodes[k].FV[l])

                        for l in range(self.PV_size):
                            #Learning
                            temp_PV[l]=self.nodes[k].PV[l]+influence*learning_rate_decaying*(input_PV[l]-self.nodes[k].PV[l])

                        #Push the unit onto stack to update in next interval
                        stack[0:0]=[[[k],temp_FV,temp_PV]]

                
                for l in range(len(stack)):
                    
                    self.nodes[stack[l][0][0]].FV[:]=stack[l][1][:]
                    self.nodes[stack[l][0][0]].PV[:]=stack[l][2][:]

                
                                    

                

    #Returns prediction vector
    def predict(self, FV=[0.0]):
        best=self.best_match(FV)
        return self.nodes[best].PV
        
    #Returns best matching unit's index
    def best_match(self, target_FV=[0.0]):

        minimum=sqrt(self.FV_size) #Minimum distance
        minimum_index=1 #Minimum distance unit
        temp=0.0
        for i in range(self.total):
            temp=0.0
            temp=self.FV_distance(self.nodes[i].FV,target_FV)
            if temp<minimum:
                minimum=temp
                minimum_index=i

        return minimum_index

    def FV_distance(self, FV_1=[0.0], FV_2=[0.0]):
        temp=0.0
        for j in range(self.FV_size):
                temp=temp+(FV_1[j]-FV_2[j])**2

        temp=sqrt(temp)
        return temp

    def distance(self, node1, node2):
        return sqrt((node1.X-node2.X)**2+(node1.Y-node2.Y)**2)

    
if __name__ == "__main__":
    print ("Initialization...")
    a=SOM(5,5,2,1,False,0.05)

    print ("Training for the XOR function...")
    a.train(100,[[[1,0],[1]],[[1,1],[0]],[[0,1],[1]],[[0,0],[0]]])

    print("Predictions for the XOR function...")

    print("Prediction 0 0,", round(a.predict([0,0])[0]))
    print("Prediction 1 0,", round(a.predict([1,0])[0]))
    print("Prediction 0 1,", round(a.predict([0,1])[0]))
    print ("Prediction 1 1,", round(a.predict([1,1])[0]))

#ART

#updated

import math
import sys

N = 4 # Number of components in an input vector.
M = 3 # Max number of clusters to be formed.
VIGILANCE = 0.4
PATTERNS = 7
TRAINING_PATTERNS = 4 # Use this many for training, the rest are for tests.

PATTERN_ARRAY = [[1, 1, 0, 0], 
                 [0, 0, 0, 1], 
                 [1, 0, 0, 0], 
                 [0, 0, 1, 1], 
                 [0, 1, 0, 0], 
                 [0, 0, 1, 0], 
                 [1, 0, 1, 0]]

class ART1_Example1:
    def __init__(self, inputSize, numClusters, vigilance, numPatterns, numTraining, patternArray):
        self.mInputSize = inputSize
        self.mNumClusters = numClusters
        self.mVigilance = vigilance
        self.mNumPatterns = numPatterns
        self.mNumTraining = numTraining
        self.mPatterns = patternArray
        
        self.bw = [] # Bottom-up weights.
        self.tw = [] # Top-down weights.

        self.f1a = [] # Input layer.
        self.f1b = [] # Interface layer.
        self.f2 = []
        return
    
    def initialize_arrays(self):
        # Initialize bottom-up weight matrix.
        sys.stdout.write("Weights initialized to:")
        for i in range(self.mNumClusters):
            self.bw.append([0.0] * self.mInputSize)
            for j in range(self.mInputSize):
                self.bw[i][j] = 1.0 / (1.0 + self.mInputSize)
                sys.stdout.write(str(self.bw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        
        sys.stdout.write("\n")
        
        # Initialize top-down weight matrix.
        for i in range(self.mNumClusters):
            self.tw.append([0.0] * self.mInputSize)
            for j in range(self.mInputSize):
                self.tw[i][j] = 1.0
                sys.stdout.write(str(self.tw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        
        sys.stdout.write("\n")
        
        self.f1a = [0.0] * self.mInputSize
        self.f1b = [0.0] * self.mInputSize
        self.f2 = [0.0] * self.mNumClusters
        return
    
    def get_vector_sum(self, nodeArray):
        total = 0
        length = len(nodeArray)
        for i in range(length):
            total += nodeArray[i]
        
        return total
    
    def get_maximum(self, nodeArray):
        maximum = 0;
        foundNewMaximum = False;
        length = len(nodeArray)
        done = False
        
        while not done:
            foundNewMaximum = False
            for i in range(length):
                if i != maximum:
                    if nodeArray[i] > nodeArray[maximum]:
                        maximum = i
                        foundNewMaximum = True
            
            if foundNewMaximum == False:
                done = True
        
        return maximum
    
    def test_for_reset(self, activationSum, inputSum, f2Max):
        doReset = False
        
        if(float(activationSum) / float(inputSum) >= self.mVigilance):
            doReset = False # Candidate is accepted.
        else:
            self.f2[f2Max] = -1.0 # Inhibit.
            doReset = True # Candidate is rejected.
        
        return doReset
    
    def update_weights(self, activationSum, f2Max):
        # Update bw(f2Max)
        for i in range(self.mInputSize):
            self.bw[f2Max][i] = (2.0 * float(self.f1b[i])) / (1.0 + float(activationSum))
        
        for i in range(self.mNumClusters):
            for j in range(self.mInputSize):
                sys.stdout.write(str(self.bw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        sys.stdout.write("\n")
        
        # Update tw(f2Max)
        for i in range(self.mInputSize):
            self.tw[f2Max][i] = self.f1b[i]
        
        for i in range(self.mNumClusters):
            for j in range(self.mInputSize):
                sys.stdout.write(str(self.tw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        sys.stdout.write("\n")
        
        return
    
    def ART1(self):
        inputSum = 0
        activationSum = 0
        f2Max = 0
        reset = True
        
        sys.stdout.write("Begin ART1:\n")
        for k in range(self.mNumPatterns):
            sys.stdout.write("Vector: " + str(k) + "\n\n")
            
            # Initialize f2 layer activations to 0.0
            for i in range(self.mNumClusters):
                self.f2[i] = 0.0
            
            # Input pattern() to f1 layer.
            for i in range(self.mInputSize):
                self.f1a[i] = self.mPatterns[k][i]
            
            # Compute sum of input pattern.
            inputSum = self.get_vector_sum(self.f1a)
            sys.stdout.write("InputSum (si) = " + str(inputSum) + "\n\n")
            
            # Compute activations for each node in the f1 layer.
            # Send input signal from f1a to the fF1b layer.
            for i in range(self.mInputSize):
                self.f1b[i] = self.f1a[i]
            
            # Compute net input for each node in the f2 layer.
            for i in range(self.mNumClusters):
                for j in range(self.mInputSize):
                    self.f2[i] += self.bw[i][j] * float(self.f1a[j])
                    sys.stdout.write(str(self.f2[i]) + ", ")
                
                sys.stdout.write("\n")
            sys.stdout.write("\n")
            
            reset = True
            while reset == True:
                # Determine the largest value of the f2 nodes.
                f2Max = self.get_maximum(self.f2)
                
                # Recompute the f1a to f1b activations (perform AND function)
                for i in range(self.mInputSize):
                    sys.stdout.write(str(self.f1b[i]) + " * " + str(self.tw[f2Max][i]) + " = " + str(self.f1b[i] * self.tw[f2Max][i]) + "\n")
                    self.f1b[i] = self.f1a[i] * math.floor(self.tw[f2Max][i])
                
                # Compute sum of input pattern.
                activationSum = self.get_vector_sum(self.f1b)
                sys.stdout.write("ActivationSum (x(i)) = " + str(activationSum) + "\n\n")
                
                reset = self.test_for_reset(activationSum, inputSum, f2Max)
            
            # Only use number of TRAINING_PATTERNS for training, the rest are tests.
            if k < self.mNumTraining:
                self.update_weights(activationSum, f2Max)
            
            sys.stdout.write("Vector #" + str(k) + " belongs to cluster #" + str(f2Max) + "\n\n")
                
        return
    
    def print_results(self):
        sys.stdout.write("Final weight values:\n")
        
        for i in range(self.mNumClusters):
            for j in range(self.mInputSize):
                sys.stdout.write(str(self.bw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        sys.stdout.write("\n")
        
        for i in range(self.mNumClusters):
            for j in range(self.mInputSize):
                sys.stdout.write(str(self.tw[i][j]) + ", ")
            
            sys.stdout.write("\n")
        sys.stdout.write("\n")
        return

if __name__ == '__main__':
    art1 = ART1_Example1(N, M, VIGILANCE, PATTERNS, TRAINING_PATTERNS, PATTERN_ARRAY)
    art1.initialize_arrays()
    art1.ART1()
    art1.print_results()
    

#old
from __future__ import (print_function,division)
import numpy as np

class ART:
    def __init__(self, n=5, m=10, rho=0.5):
         
         self.F1 = np.ones(n) 
         self.F2 = np.ones(m) 
         self.Wf = np.random.random((m,n))
         self.Wb = np.random.random((n,m))
         self.rho = rho
         self.active = 0 

    def learn(self, X):
         self.F2[...] = np.dot(self.Wf, X)
         l = np.argsort(self.F2[:self.active].ravel())[::-1]

         for i in l:
               d = (self.Wb[:,i]*X).sum()/X.sum()
               if d >= self.rho:
                    self.Wb[:,i] *= X
                    self.Wf[i,:] = self.Wb[:,i]/(0.5+self.Wb[:,i].sum())
                    return self.Wb[:,i], i

         if self.active < self.F2.size:
             i = self.active
             self.Wb[:,i] *= X
             self.Wf[i,:] = self.Wb[:,i]/(0.5+self.Wb[:,i].sum())
             self.active += 1
             return self.Wb[:,i], i
         return None,None
if __name__ == '__main__':
 np.random.seed(1)
 network = ART( 5, 10, rho=0.5)
 data = ["   O ",
      "  O O",
      "    O",
      "  O O",
      "    O",
      "  O O",
      "    O",
      " OO O",
      " OO  ",
      " OO O",
      " OO  ",
      "OOO  ",
      "OO   ",
      "O    ",
      "OO   ",
      "OOO  ",
      "OOOO ",
      "OOOOO",
      "O    ",
      " O   ",
      "  O  ",
      "   O ",
      "    O",   
      "  O O",
      " OO O",
      " OO  ",
      "OOO  ",
      "OO   ", 
      "OOOO ",
      "OOOOO"]        
 X = np.zeros(len(data[0]))
 for i in range(len(data)):
     for j in range(len(data[i])):
         X[j] = (data[i][j] == 'O')
     Z ,k =network.learn(X)
     print("|%s|"%data[i],"-> class", k)


#Linear Sepration
import numpy as np
import matplotlib.pyplot as plt 
def create_distance_function(a, b, c):
    """ 0 = ax + by + c """
    def distance(x, y):
        """ returns tuple (d, pos)
            d is the distance
            If pos == -1 point is below the line, 
            0 on the line and +1 if above the line
        """
        nom = a * x + b * y + c
        if nom == 0:
            pos = 0
        elif (nom<0 and b<0) or (nom>0 and b>0):
            pos = -1
        else:
            pos = 1
        return (np.absolute(nom) / np.sqrt( a ** 2 + b ** 2), pos)
    return distance
    
points = [ (3.5, 1.8), (1.1, 3.9) ]
fig, ax = plt.subplots()
ax.set_xlabel("sweetness")
ax.set_ylabel("sourness")
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 8])
X = np.arange(-0.5, 5, 0.1)
colors = ["r", ""] # for the samples
size = 10
for (index, (x, y)) in enumerate(points):
    if index== 0:
        ax.plot(x, y, "o", 
                color="darkorange", 
                markersize=size)
    else:
        ax.plot(x, y, "oy", 
                markersize=size)
step = 0.05
for x in np.arange(0, 1+step, step):
    slope = np.tan(np.arccos(x))
    dist4line1 = create_distance_function(slope, -1, 0)
    #print("x: ", x, "slope: ", slope)
    Y = slope * X
    
    results = []
    for point in points:
        results.append(dist4line1(*point))
    #print(slope, results)
    if (results[0][1] != results[1][1]):
        ax.plot(X, Y, "g-")
    else:
        ax.plot(X, Y, "r-")
        
plt.show()

#Associative memory

import matplotlib.pyplot as plt
import numpy as np

# Set random seed for reproducibility
np.random.seed(1000)

nb_patterns = 4
pattern_width = 4
pattern_height = 4
max_iterations = 10

# Initialize the patterns
X = np.zeros((nb_patterns, pattern_width * pattern_height))

X[0] = [-1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1]
X[1] = [-1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1]
X[2] = [-1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1]
X[3] = [1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1]

# Show the patterns
fig, ax = plt.subplots(1, nb_patterns, figsize=(10, 5))

for i in range(nb_patterns):
    ax[i].matshow(X[i].reshape((pattern_height, pattern_width)), cmap='gray')
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    
plt.show()

# Train the network
W = np.zeros((pattern_width * pattern_height, pattern_width * pattern_height))

for i in range(pattern_width * pattern_height):
    for j in range(pattern_width * pattern_height):
        if i == j or W[i, j] != 0.0:
            continue
            
        w = 0.0
        
        for n in range(nb_patterns):
            w += X[n, i] * X[n, j]
            
        W[i, j] = w / X.shape[0]
        W[j, i] = W[i, j]
        
# Create a corrupted test pattern
x_test = np.array([1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1])

# Recover the original patterns
A = x_test.copy()

for _ in range(max_iterations):
    for i in range(pattern_width * pattern_height):
        A[i] = 1.0 if np.dot(W[i], A) > 0 else -1.0

# Show corrupted and recovered patterns
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

ax[0].matshow(x_test.reshape(pattern_height, pattern_width), cmap='gray')
ax[0].set_title('Corrupted pattern')
ax[0].set_xticks([])
ax[0].set_yticks([])

ax[1].matshow(A.reshape(pattern_height, pattern_width), cmap='gray')
ax[1].set_title('Recovered pattern')
ax[1].set_xticks([])
ax[1].set_yticks([])

plt.show()


#Membership IN operator
list1=[]
print("Enter 5 numbers")
for i in range(0,5):
    v=input()
    list1.append(v)
list2=[]
print("Enter 5 numbers")
for i in range(0,5):
    v=input()
    list2.append(v)
flag=0
for i in list1:
    if i in list2:
        flag=1
if(flag==1):
    print("The Lists Overlap")
else:
    print("The Lists do Not overlap")

#Membership Not IN operator
list1=[]
print("Enter 5 numbers")
for i in range(0,5):
    v=input()
    list1.append(v)
list2=[]
print("Enter 5 numbers")
for i in range(0,5):
    v=input()
    list2.append(v)
flag=0
print("The elements in the first list not in second list are")
for i in list1:
    if i not in list2:
        print(i)
		
#Membership Is operator

# Python program to illustrate the use 
# of 'is' identity operator 
x = 5
if (type(x) is int): 
	print ("true") 
else: 
	print ("false")

#Membership Is operator

# Python program to illustrate the 
# use of 'is not' identity operator 
x = 5.2
if (type(x) is not  int): 
	print ("true") 
else: 
	print ("false")

#Find ratios using Fuzzy
from fuzzywuzzy import fuzz 
from fuzzywuzzy import process 
print("Name: Mohammed K")
print("Roll No: 14")

s1 = "I love fuzzysforfuzzys"
s2 = "I am loving fuzzysforfuzzys"
print ("FuzzyWuzzy Ratio:", fuzz.ratio(s1, s2)) 
print ("FuzzyWuzzyPartialRatio: ", fuzz.partial_ratio(s1, s2)) 
print ("FuzzyWuzzyTokenSortRatio: ", fuzz.token_sort_ratio(s1, s2)) 
print ("FuzzyWuzzyTokenSetRatio: ", fuzz.token_set_ratio(s1, s2)) 
print ("FuzzyWuzzyWRatio: ", fuzz.WRatio(s1, s2),'\n\n')

# for process library, 
query = 'fuzzys for fuzzys'
choices = ['fuzzy for fuzzy', 'fuzzy fuzzy', 'g. for fuzzys'] 
print ("List of ratios: ")
print (process.extract(query, choices), '\n')
print ("Best among the above list: ",process.extractOne(query, choices))


#GA
import random 
# Number of individuals in each generation 
POPULATION_SIZE = 100
  # Valid genes 
GENES = '''abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOP 
QRSTUVWXYZ 1234567890, .-;:_!"#%&/()=?@${[]}'''
  # Target string to be generated 
TARGET = "I love GeeksforGeeks"
class Individual(object): 
    ''' 
    Class representing individual in population 
    '''
    def __init__(self, chromosome): 
        self.chromosome = chromosome  
        self.fitness = self.cal_fitness() 

    @classmethod
    def mutated_genes(self): 
        ''' 
        create random genes for mutation 
        '''
        global GENES 
        gene = random.choice(GENES) 
        return gene 

    @classmethod
    def create_gnome(self): 
        ''' 
        create chromosome or string of genes 
        '''
        global TARGET 
        gnome_len = len(TARGET) 
        return [self.mutated_genes() for _ in range(gnome_len)] 

    def mate(self, par2): 
        ''' 
        Perform mating and produce new offspring 
        '''

        # chromosome for offspring 
        child_chromosome = [] 
        for gp1, gp2 in zip(self.chromosome, par2.chromosome):     

            # random probability   
            prob = random.random() 

            # if prob is less than 0.45, insert gene 
            # from parent 1  
            if prob < 0.45: 
                child_chromosome.append(gp1) 

            # if prob is between 0.45 and 0.90, insert 
            # gene from parent 2 
            elif prob < 0.90: 
                child_chromosome.append(gp2) 

            # otherwise insert random gene(mutate),  
            # for maintaining diversity 
            else: 
                child_chromosome.append(self.mutated_genes()) 

        # create new Individual(offspring) using  
        # generated chromosome for offspring 
        return Individual(child_chromosome) 

    def cal_fitness(self): 
        ''' 
        Calculate fittness score, it is the number of 
        characters in string which differ from target 
        string. 
        '''
        global TARGET 
        fitness = 0
        for gs, gt in zip(self.chromosome, TARGET): 
            if gs != gt: fitness+= 1
        return fitness 

# Driver code 
def main(): 
    global POPULATION_SIZE 

    #current generation 
    generation = 1

    found = False
    population = [] 

    # create initial population 
    for _ in range(POPULATION_SIZE): 
                gnome = Individual.create_gnome() 
                population.append(Individual(gnome)) 

    while not found: 

        # sort the population in increasing order of fitness score 
        population = sorted(population, key = lambda x:x.fitness) 

        # if the individual having lowest fitness score ie.  
        # 0 then we know that we have reached to the target 
        # and break the loop 
        if population[0].fitness <= 0: 
            found = True
            break

        # Otherwise generate new offsprings for new generation 
        new_generation = [] 

        # Perform Elitism, that mean 10% of fittest population 
        # goes to the next generation 
        s = int((10*POPULATION_SIZE)/100) 
        new_generation.extend(population[:s]) 

        # From 50% of fittest population, Individuals  
        # will mate to produce offspring 
        s = int((90*POPULATION_SIZE)/100) 
        for _ in range(s): 
            parent1 = random.choice(population[:50]) 
            parent2 = random.choice(population[:50]) 
            child = parent1.mate(parent2) 
            new_generation.append(child) 

        population = new_generation 

        print("Generation: {}\tString: {}\tFitness: {}". format(generation, "".join(population[0].chromosome), 
              population[0].fitness)) 

        generation += 1


    print("Generation: {}\tString: {}\tFitness: {}". format(generation, 
          "".join(population[0].chromosome), 
          population[0].fitness)) 

if __name__ == '__main__': 
    main() 
print("Name: Bidlan Aakash")
print("Roll No: 45")    


